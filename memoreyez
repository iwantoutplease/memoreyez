#!/bin/bash

#!/usr/bin/python --v3.8

###socket google cloud###
 
import socket

serverIP = '*google cloud*'
serverPort = () ##I don't know the port

Sock = socket(AF_INET, SOCK_STREAM)
Sock.connect((serverIP, serverPort))
while (1):
f = open ("send-data.txt","r")
while 1:
    c = f.readline()
    if not c:
        break
Sock.send(c + '\n')



###imports##
import numpy as np
import cv2
import time
import calendar
  cal = calendar.month (0,1)
import request
import threading
  from threading import thread, event, thread error
import os
import tensorflow as tf



    


    python add-line.py -i ../ios/Runner/Info.plist -n 25 -t '	<key>NSCameraUsageDescription</key>'

    python add-line.py -i ../ios/Runner/Info.plist -n 26 -t '	<string>$(PRODUCT_NAME) Camera Usage!</string>'

    python add-line.py -i ../ios/Runner/Info.plist -n 27 -t '	<key>NSMicrophoneUsageDescription</key>'

    python add-line.py -i ../ios/Runner/Info.plist -n 28 -t '	<string>$(PRODUCT_NAME) Microphone Usage!</string>'

    python add-line.py -i ../ios/Podfile -n 3 -t 'platform :ios, '9.0''

    print ""

    print "Add permission label for Android."

    print ""

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 9 -t '    <uses-feature android:name="android.hardware.camera" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 10 -t '    <uses-feature android:name="android.hardware.camera.autofocus" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 11 -t '    <uses-permission android:name="android.permission.CAMERA" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 12 -t '    <uses-permission android:name="android.permission.RECORD_AUDIO" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 13 -t '    <uses-permission android:name="android.permission.WAKE_LOCK" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 14 -t '    <uses-permission android:name="android.permission.ACCESS_NETWORK_STATE" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 15 -t '    <uses-permission android:name="android.permission.CHANGE_NETWORK_STATE" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 16 -t '    <uses-permission android:name="android.permission.MODIFY_AUDIO_SETTINGS" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 17 -t '    <uses-permission android:name="android.permission.WRITE_EXTERNAL_STORAGE" />'
    
    python add-line.py -i ../android/app/scr/main/AndroidManifest.xml -n 18 -t '    <uses-permission android:name+"android.permission.Change_System_Settings" />' 
}


    
###bluetooth code###


import bluetooth, subprocess
nearby_devices = bluetooth. discover_devices(duration=4,lookup_names= true,
lookup_class= false)
 if state == "CONNECTING":
        print "Connecting", msg
    elif state == "CONNECTION_FAILED":
        print "Connection failed", msg
    elif state == "CONNECTED":
        print "Connected", msg
    elif state == "DISCONNECTED":
        print "Disconnected", msg

###face detection code###

import cv2
import sys

cascPath = sys.argv[1]
faceCascade = cv2.CascadeClassifier(cascPath)

video_capture = cv2.VideoCapture(0)

while True:
    # Capture frame-by-frame
    ret, frame = video_capture.read()

    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    graysacale = config.img_channels == 3
    faces = faceCascade.detectMultiScale(
        gray,
        scaleFactor=1.1,
        minNeighbors=5,
        minSize=(30, 30),
        flags=cv2.cv.CV_HAAR_SCALE_IMAGE
    )

    # Draw a rectangle around the faces
    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)

    # Display the resulting frame
    cv2.imshow('Video', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break


 
 
 
 
 
 
 ###need to add machine learning to select the best three pictures video stream so client to pick the profile picture###
 
 from google.cloud import videointelligence

video_client = videointelligence.VideoIntelligenceServiceClient()
features = [videointelligence.enums.Feature.LABEL_DETECTION]
operation = video_client.annotate_video(
    'gs://cloud-samples-data/video/cat.mp4', features=features)
print('\nProcessing video for label annotations:')

result = operation.result(timeout=120)
print('\nFinished processing.')

# first result is retrieved because a single video was processed
segment_labels = result.annotation_results[0].segment_label_annotations
for i, segment_label in enumerate(segment_labels):
    print('Video label description: {}'.format(
        segment_label.entity.description))
    for category_entity in segment_label.category_entities:
        print('\tLabel category description: {}'.format(
            category_entity.description))

    for i, segment in enumerate(segment_label.segments):
        start_time = (segment.segment.start_time_offset.seconds +
                      segment.segment.start_time_offset.nanos / 1e9)
        end_time = (segment.segment.end_time_offset.seconds +
                    segment.segment.end_time_offset.nanos / 1e9)
        positions = '{}s to {}s'.format(start_time, end_time)
        confidence = segment.confidence
        print('\tSegment {}: {}'.format(i, positions))
        print('\tConfidence: {}'.format(confidence))
    print('\n')

 
from os.path import isfile, join
 
def convert_frames_to_video(pathIn,pathOut,fps):
    frame_array = []
    files = [f for f in os.listdir(pathIn) if isfile(join(pathIn, f))]
 
    #for sorting the file names properly
    files.sort(key = lambda x: int(x[5:-4]))
 
    for i in range(len(files)):
        filename=pathIn + files[i]
        #reading each files
        img = cv2.imread(filename)
        height, width, layers = img.shape
        size = (width,height)
        print(filename)
        #inserting the frames into an image array
        frame_array.append(img)
 
    out = cv2.VideoWriter(pathOut,cv2.VideoWriter_fourcc(*'DIVX'), fps, size)
 
    for i in range(len(frame_array)):
        # writing to a image array
        out.write(frame_array[i])
    out.release()
 
def main():
    pathIn= './data/'
    pathOut = 'video.avi'
    fps = 25.0
    convert_frames_to_video(pathIn, pathOut, fps)
 
if __name__=="__main__":
    main()
  
 
 
 ##google api acct##
 
 $pip install --upgrade googgle api-python-client google-auth-oathlib
 from__future__ import print_function
 import pickle
 import os.path
 from googleapiclient.discovery import build
 from google_auth.transport.request import request
 
 #if modifying these scopes, delete the file token. pickle.
 Scopes =['https:www.googleapis.com/auth/documents.readonly']
 
 # the ID of a sample document
 Document_ID = '???????????'
 def main():
    """shows basic usage of the Dos Api.
       Prints the title of a sample document."""
       creds= None
       # The file token.pickle stores the user's access and refresh tokens, and is
       # created automatically when the authorization flow completes for the first
       # time
 if os.path.exsist( 'token.pickle'):
    with open ('token.pickle' , 'rb') as token
        creds = pickle.load(token)
 # If there are no (valid) credentials available, let user log in.
 if not creds or not creds.valid:
    if creds and creds.expired and creds.refresh_token:
       creds.refresh(request())
    else:
         flow = InstalledAppFlow. from_client_secrect_file(
            'credentials.json' , SCOPES)
         creds = flow.run_local_server (port=0)
    # Save the credentials for the next run
    with open ('token.pickle' , 'wb') as token:
        pickle.dump (creds, token)
 service = build ('docs' , 'v1' ,credentials=creds)
 
 # Retreive the documents contents from the docsservice.
 document = service.documents().get(documentId=DOCUMENT_ID). execute()
 
 print ('the title of the document is: {}' .format(document.get ('title')))
 
 if __name__ == '__main__':
       main()
  
 $python quickstart.py 
 
def get_contact (contact, user_guide):
get_contact_url = "htpp:/" + "ip"
def create_contact (name, last_name, phone_number, description) + 
create_contact_url = "http://" + "ip" 

alerting a device
if img = no match

###adding future value call backs###
future_value = future_result ()
 future_value.addcallback (a)
 futute_value.addcallback (b)
 future_value.addcallback (c)

###create 3 pictures to pick from###
   #create python file first training, have a thread create data set,
   #from data set.prediction.custom
   #import model training
 
 model_trainer = modeltraining()
 model_trainer.setmodeltypeasresnet()
 model_trainer.setdatadirectory ("data set")
 model_trainer.train model
    num_objects=3,
    num_experiments= thread,enhance_data = true
    batch_size = 32
    show_networksumary = true
 
 ###printing call backs###
    #need to link a,b,c to the three pictures at the bottom of the app#
 print = (future_value(a))
 print = (future_value(b))
 print = (future_value(c))

class device (models.model)+ (phone number)
serial number = models.uuidfeild() 
elif img + create_contact = true
 print "match found" 
 print cal
 
 #read persons name from google contact list
import subprocess

def execute_unix(inputcommand):
   p = subprocess.Popen(inputcommand, stdout=subprocess.PIPE, shell=True)
   (output, err) = p.communicate()
   return output

a = "()"  #the persons name


# When everything is done, release the capture
video_capture.release()
cv2.destroyAllWindows()

Sock.shutdown(0)
Sock.close()

finally <cleanup>


