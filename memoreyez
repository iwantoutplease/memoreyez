#!/usr/bin/python --v3.8

###socket to a cloud###
 
import socket

serverIP = '**'
serverPort = () ##I don't know the port

Sock = socket(AF_INET, SOCK_STREAM)
Sock.connect((serverIP, serverPort))
while (1):
f = open ("send-data.txt","r")
while 1:
    c = f.readline()
    if not c:
        break
Sock.send(c + '\n')



###imports##
import numpy as np
import cv2
import time
import calendar
  cal = calendar.month (0,1)
import request
import threading
  from threading import thread, event, thread error
import os
import tensorflow as tf


###variables###

fn = ('first_name')
ln = ('last_name')
cv = ('customers_voice')


    


    python add-line.py -i ../ios/Runner/Info.plist -n 25 -t '	<key>NSCameraUsageDescription</key>'

    python add-line.py -i ../ios/Runner/Info.plist -n 26 -t '	<string>$(PRODUCT_NAME) Camera Usage!</string>'

    python add-line.py -i ../ios/Runner/Info.plist -n 27 -t '	<key>NSMicrophoneUsageDescription</key>'

    python add-line.py -i ../ios/Runner/Info.plist -n 28 -t '	<string>$(PRODUCT_NAME) Microphone Usage!</string>'

    python add-line.py -i ../ios/Podfile -n 3 -t 'platform :ios, '9.0''

    print ""

    print "Add permission label for Android."

    print ""

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 9 -t '    <uses-feature android:name="android.hardware.camera" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 10 -t '    <uses-feature android:name="android.hardware.camera.autofocus" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 11 -t '    <uses-permission android:name="android.permission.CAMERA" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 12 -t '    <uses-permission android:name="android.permission.RECORD_AUDIO" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 13 -t '    <uses-permission android:name="android.permission.WAKE_LOCK" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 14 -t '    <uses-permission android:name="android.permission.ACCESS_NETWORK_STATE" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 15 -t '    <uses-permission android:name="android.permission.CHANGE_NETWORK_STATE" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 16 -t '    <uses-permission android:name="android.permission.MODIFY_AUDIO_SETTINGS" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 17 -t '    <uses-permission android:name="android.permission.WRITE_EXTERNAL_STORAGE" />'
    
    python add-line.py -i ../android/app/scr/main/AndroidManifest.xml -n 18 -t '    <uses-permission android:name+"android.permission.Change_System_Settings" />' 
}

###voice recognition###

##voice recognition to identify who is talking###
  #This will be used as intial setup for the app#

import glob
import numpy as np
import random
import names from pypi
import librosa
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import labelBinarizer

import keras
from keras.layers import LSTM, dense, dropout, flatten
from keras.models import sequential
from keras.optimozers import adam
from keras.callbacks import Earlystopping, ModelCheckpoint


###need to add a button to be pressed to save customers voice/name###


print ("say your first and last name")



seed = 2020 ###2017 is given in the exmple and may need to be changed to that###
Data_Dir = 'data/spoken_numbers_pcm/'

files = glob. glob(Data_Dir + "*.wav")
X_train, X_val = train_test_split(files, test_size=0.2, random_state = SEED)

print ('# Traning examples: {} '.format(len(X_train)))
print ('# Validation examples: {}'.format (len(X_val)))

labels = []
   for i in range (len(X_train)):
        label = X_train [i].split('/'[-1].split('_')[1]
        if label not in labels:
             labels.append(label)
print (labels)

label_binarizer = labelbinarizer()
label_binarizer .fit(list(set(labels)))

def one_hot_enode(x): return label_binarizer.transform(x)

n_features = 20
max_length = 80
n_classes = len(labels)

f batch_generator(data, batch_size=16)
   while 1:
       random.shuffle(data)
       x, y = [] , []
       for i in range(batch_size):
           wav = data [i]
           wave, sr = librosa.load(wav, mono = true)
           label = wav.split ('/')[-1].split('_')[1]
           y.append(one_hot_encode(label))
           mfcc = librosa.feature.mfcc(wave , sr)
           mfcc = np.pad(mfcc, ((0,0), (0, max_length-
           len(mfcc[0]))), mode = 'constant',constant_value=0)
           x.append(np.array(mfcc))
     yield np.array(x), np.array(y)
     
     
 learning_rate = 0.001
 batch_size = 64
 n_epochs = 50
 dropout = 0.5
 
 input_shape = (n_features, max_length)
 steps_per_epoch = 50
 
 model = Sequential()
 model.add (LSTM(256, return_sequences=true,
    dropout=dropout))
 model.add(flattten())
 model.add(Dense(128, activation= 'relu'))
 model.add(Dropout(dropout))
 model.add(Dense(n_classes, activation='softmax'))
 
 opt = Adam(lr=learning_rate)
   model.compile(loss='catogorical_crosstropy', optimizer= opt,
   metrcs=['accuracy'])
   model.summary()
   
 callbacks = [ModelCheckpoint('checkpoints/voice_recognition_best_model_{epoch:02d}.hdfS' , save_best_only=True).
 
 history = model.fit_generator(
    generator=batch_generator(X_train, batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=n_epochs,
    verbose=1
    validation_data=batch_generator(X_val, 32)
    validation_steps=5
    calllbacks=callbacks)
    
###Save owner's voice to be used as a false input for future data###

 
    
###bluetooth code###


import bluetooth, subprocess
nearby_devices = bluetooth. discover_devices(duration=4,lookup_names= true,
lookup_class= false)
 if state == "CONNECTING":
        print "Connecting", msg
    elif state == "CONNECTION_FAILED":
        print "Connection failed", msg
    elif state == "CONNECTED":
        print "Connected", msg
    elif state == "DISCONNECTED":
        print "Disconnected", msg
        
        
        
### ignoring customer' voice###

if cv = true then ('fn','ln') = false
elif cv = false then ('fn','ln') = true
              

              
###speech recognition###
  #used to identify the contacts name#

              
import speech_recognition as sr from pycharm
import names from pypi
r1= sr.recognizer()
r2= sr.recognizer()


 with sr.microphone() as source:
   >>>names.get_first_name()
     print('fn') #first name found in the stream
    
     audio = r1.listen(source)
    
 with sr.microphone() as source:
    >>>names.get_last_name()
     
      print('ln') #last name found in the stream
      audio = r2.listen (source)
      
 
$pip install tox coverage
$./runtest.sh





###face detection code###

import cv2
import sys

cascPath = sys.argv[1]
faceCascade = cv2.CascadeClassifier(cascPath)

video_capture = cv2.VideoCapture(0)

while True:
    # Capture frame-by-frame
    ret, frame = video_capture.read()

    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    graysacale = config.img_channels == 3
    faces = faceCascade.detectMultiScale(
        gray,
        scaleFactor=1.1,
        minNeighbors=5,
        minSize=(30, 30),
        flags=cv2.cv.CV_HAAR_SCALE_IMAGE
    )

    # Draw a rectangle around the faces
    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)

    # Display the resulting frame
    cv2.imshow('Video', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break


 
 
 
 
 
 
 ###need to add machine learning to select the best three pictures video stream so client to pick the profile picture###
 
   
 $python quickstart.py 
 
def get_contact (contact, user_guide):
get_contact_url = "htpp:/" + "ip"
def create_contact (name, last_name, phone_number, description) + 
create_contact_url = "http://" + "ip" 

alerting a device
if img = no match

###adding future value call backs###
future_value = future_result ()
 future_value.addcallback (a)
 futute_value.addcallback (b)
 future_value.addcallback (c)

###create 3 pictures to pick from###
   #create python file first training, have a thread create data set,
   #from data set.prediction.custom
   #import model training
 
 model_trainer = modeltraining()
 model_trainer.setmodeltypeasresnet()
 model_trainer.setdatadirectory ("data set")
 model_trainer.train model
    num_objects=3,
    num_experiments= thread,enhance_data = true
    batch_size = 32
    show_networksumary = true
 
 ###printing call backs###
    #need to link a,b,c to the three pictures at the bottom of the app#
 print = (future_value(a))
 print = (future_value(b))
 print = (future_value(c))

class device (models.model)+ (phone number)
serial number = models.uuidfeild() 
elif img + create_contact = true
 print "match found" 
 print cal
 
 #read persons name from google contact list
import subprocess

def execute_unix(inputcommand):
   p = subprocess.Popen(inputcommand, stdout=subprocess.PIPE, shell=True)
   (output, err) = p.communicate()
   return output

fn= "()"  #first name
ln= "()" #last name


# When everything is done, release the capture
video_capture.release()
cv2.destroyAllWindows()

Sock.shutdown(0)
Sock.close()

finally <cleanup>


