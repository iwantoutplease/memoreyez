#!/bin/bash

#!/usr/bin/python --v3.8

###socket google cloud###
 #will need to create multiple sockets for different libraries in google cloud#

import socket

serverIP = '*google cloud*'
serverPort = () ##I don't know the port

Sock = socket(AF_INET, SOCK_STREAM)
Sock.connect((serverIP, serverPort))
while (1):
f = open ("send-data.txt","r")
while 1:
    c = f.readline()
    if not c:
        break
Sock.send(c + '\n')

###connecting to google library###

from oauth2client.client import GoogleCredentials
from googleapiclient import discovery
from googleapiclient import errors
 ml = discovery.build('ml','v1')
 project_id = 'projects/{}'.format('memoreyez')
 request_dict = {'name': 'memoreyez',
               'description': 'This is a machine learning model entry.'}
  request = ml.projects().models().create(parent=project_id, body=request_dict)
  response = request.execute()
  response = ml.projects().models().create(parent=project_id,
                                         body=request_dict).execute()
  try:
    response = request.execute()
    print(response)
    except errors.HttpError, err:
    # Something went wrong, print out some information.
    print('There was an error creating the model. Check the details:')
    print(err._get_reason())
  from googleapiclient import discovery
from googleapiclient import errors

# Store your full project ID in a variable in the format the API needs.
project_id = 'projects/{}'.format('your_project_ID')

# Build a representation of the Cloud ML API.
ml = discovery.build('ml', 'v1')

# Create a dictionary with the fields from the request body.
request_dict = {'name': 'your_model_name',
               'description': 'your_model_description'}

# Create a request to call projects.models.create.
request = ml.projects().models().create(
              parent=project_id, body=request_dict)

# Make the call.
try:
    response = request.execute()
    print(response)
except errors.HttpError, err:
    # Something went wrong, print out some information.
    print('There was an error creating the model. Check the details:')
    print(err._get_reason())
request = ml.projects().models().versions().delete(
    name='projects/myproject/models/mymodel/versions/myversion')
response = request.execute()

some_value = response.get('some_key') or 'default_value'


###imports##
import numpy as np
import cv2
import time
import calendar
  cal = calendar.month (0,1)
import request
import threading
  from threading import thread, event, thread error
import os
import tensorflow as tf

###app/website permissions###
memoreyez_APP_FOLDER=$(https://github.com/iwantoutplease/memoreyez/blob/master/Memoreyez.iml)

memoreyez_APP_ORG=https://github.com/iwantoutplease/memoreyez/blob/master/Memoreyez.iml

MEMOREYEZ_APP_PROJECT_NAME=ion_sfu_example

CMD=$1


 cleanup() {

    print "Cleanup project [$MEMOREYEZ_APP_PROJECT_NAME] files ..."

    cd $MEMOREYEZ_APP_FOLDER

    rm -rf android build *.iml ios pubspec.lock test .??????-plugins .metadata .packages .idea  }





 create() {

    cd $MEMOREYEZ_APP_FOLDER

     [ ! -d "ios" ] && [ ! -d "android" ];then

        print "Create Memoreyez project: name=$MEMOREYEZ_APP_PROJECT_NAME, org=$MEMOREYEZ_APP_ORG ..."

        flutter create --project-name $MEMOREYEZ_APP_PROJECT_NAME --org $MEMOREYEZ_APP_ORG .

        add_permission_label

    

        print "Project [$MEMOREYEZ_APP_PROJECT_NAME] already exists!"  }  



 add_permission_label() {

    cd $MEMOREYEZ_APP_FOLDER/scripts

    print ""

    print "Add permission label for iOS."

    print ""

    python add-line.py -i ../ios/Runner/Info.plist -n 25 -t '	<key>NSCameraUsageDescription</key>'

    python add-line.py -i ../ios/Runner/Info.plist -n 26 -t '	<string>$(PRODUCT_NAME) Camera Usage!</string>'

    python add-line.py -i ../ios/Runner/Info.plist -n 27 -t '	<key>NSMicrophoneUsageDescription</key>'

    python add-line.py -i ../ios/Runner/Info.plist -n 28 -t '	<string>$(PRODUCT_NAME) Microphone Usage!</string>'

    python add-line.py -i ../ios/Podfile -n 3 -t 'platform :ios, '9.0''

    print ""

    print "Add permission label for Android."

    print ""

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 9 -t '    <uses-feature android:name="android.hardware.camera" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 10 -t '    <uses-feature android:name="android.hardware.camera.autofocus" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 11 -t '    <uses-permission android:name="android.permission.CAMERA" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 12 -t '    <uses-permission android:name="android.permission.RECORD_AUDIO" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 13 -t '    <uses-permission android:name="android.permission.WAKE_LOCK" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 14 -t '    <uses-permission android:name="android.permission.ACCESS_NETWORK_STATE" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 15 -t '    <uses-permission android:name="android.permission.CHANGE_NETWORK_STATE" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 16 -t '    <uses-permission android:name="android.permission.MODIFY_AUDIO_SETTINGS" />'

    python add-line.py -i ../android/app/src/main/AndroidManifest.xml -n 17 -t '    <uses-permission android:name="android.permission.WRITE_EXTERNAL_STORAGE" />'
    
    python add-line.py -i ../android/app/scr/main/AndroidManifest.xml -n 18 -t '    <uses-permission android:name+"android.permission.Change_System_Settings" />' 
}



 [ "$CMD" == "create" ];



    create

 [ "$CMD" == "cleanup" ];

    cleanup


 [ "$CMD" == "add_permission" ];


    add_permission_label

  [ ! -n "$1" ] ;then

    print "Usage: ./project_tools.sh 'create' | 'cleanup'"
    
    
###bluetooth code###


import bluetooth, subprocess
nearby_devices = bluetooth. discover_devices(duration=4,lookup_names= true,
lookup_class= false)
 if state == "CONNECTING":
        print "Connecting", msg
    elif state == "CONNECTION_FAILED":
        print "Connection failed", msg
    elif state == "CONNECTED":
        print "Connected", msg
    elif state == "DISCONNECTED":
        print "Disconnected", msg


###small camera code###

class cam ():
  def__init__(self,url):
   self.stream = request.get(url,stream)
   self.thread_cancelled = ()
   self.thread = thread(target = self,run)
   print "camera initialised"
   
  def start (self):
   self.thread.start()
   print "camera stream started"
   
  def run (self):
   bytes = ''
    not self.thread_cancelled:
    try:
    bytes+=self.stream.raw.read()
    a = bytes.find ('\xff\xd8')
    b = bytes.find ('\xff\xd9')
    a!=-1 and b!=-1:
    jpg = bytes [a:b:]
    
    cv2.imashow('cam',img)


# get user supplied values#
   image path = sys.argv[1]
   cascpath = sys.argv[2]
# create the haar cascade
   facecascade = cv2.cascadeclassifier (cascpath)
# read the image
   image = cv2.imread (imagepath)
   gray = cv2.cvtcolor (image, cv2.color_bgr2gray)
# detect faces in the image
    faces = facecascade.detectmultiscale(gray,scalefactor=1.1,minNeighbors=5,minSize=(30, 30),flags = cv2.cv. CV_HAAR_SCALE_IMAGE)
    print "found {0} faces!".format (len(faces))
# draw a rectangle around the face
    (x,y,w,h) faces: 
   cv2 rectangle (image,(x,y),(x+w,y+h), (0,255,0),2)
   cv2.imshow("faces found", image)
   cv2.waitkey(0)
   
   $python face_detect.py abba.png haarcascade_frontalface_default.xml -- this is to check a shell...(https://realpython.com)
 
 ###need to add lip reading software with usage of microphone to insert correct name machine learning###
 
 duration = 5 # minutes
 myrecording = sd.rec (int(duration + fs) . samplerate=fs. channels=1)
 sd.default.samplerate = fs
 sd.default.channels=1
 sd.wait()
stream.read()



from config import load_args

from data.label_vectorization import SentenceVectorizer

from data.load_video import load_video_frames



config = load_args()

class ListGenerator:



  def __iter__(self):

    return self



  def __init__(self, data_list):



    self.lock = threading.Lock()



    self.batch_size = config.batch_size



    self.data_path = config.data_path



    self.all_samples = np.loadtxt(data_list, str, delimiter=', ')

    if self.all_samples.ndim == 1:

    self.all_samples = self.all_samples[None,:] # only one sample, expand batch dim

    assert self.all_samples.size > 0, "No samples found, please check the paths"

    self._tot_samps = len(self.all_samples)

    print("Found {} samples".format(self._tot_samps))



    self.v_idx = 0



    self.label_vectorizer = SentenceVectorizer(label_maxlen=config.maxlen,

            transcribe_digits=config.transcribe_digits)



  def calc_nbatches_per_epoch(self):

    return self.__len__()//self.batch_size



  def __len__(self):

    return self._tot_samps



  def next(self):



    frames_batch = []

    labels_batch = []



    video_frames = []

    cnt = 0

    while cnt< self.batch_size:

      with self.lock:

        vid, label = self.all_samples[self.v_idx]

        self.v_idx += 1

      frames = load_video_frames( os.path.join(self.data_path, vid),

             maxlen=config.maxlen,

             pad_mode=config.pad_mode,

             grayscale=config.img_channels == 3

                    
            video_frames.append(frames)

            labels_batch.append(label)

            cnt+=1



    assert len(video_frames) == self.batch_size

    video_frames = np.stack(video_frames, axis = 0)

    labels_batch = [self.label_vectorizer.vectorize(labels_batch)]

    frames_batch = [video_frames]



    return [frames_batch, labels_batch]



  def strip_extension(self,path):

    _, file_extension = os.path.splitext(path)

    path = path.replace(file_extension,'')

    return path 
 
 ###need to add machine learning to select the best three pictures video stream so client to pick the profile picture###
 
 from google.cloud import videointelligence

video_client = videointelligence.VideoIntelligenceServiceClient()
features = [videointelligence.enums.Feature.LABEL_DETECTION]
operation = video_client.annotate_video(
    'gs://cloud-samples-data/video/cat.mp4', features=features)
print('\nProcessing video for label annotations:')

result = operation.result(timeout=120)
print('\nFinished processing.')

# first result is retrieved because a single video was processed
segment_labels = result.annotation_results[0].segment_label_annotations
for i, segment_label in enumerate(segment_labels):
    print('Video label description: {}'.format(
        segment_label.entity.description))
    for category_entity in segment_label.category_entities:
        print('\tLabel category description: {}'.format(
            category_entity.description))

    for i, segment in enumerate(segment_label.segments):
        start_time = (segment.segment.start_time_offset.seconds +
                      segment.segment.start_time_offset.nanos / 1e9)
        end_time = (segment.segment.end_time_offset.seconds +
                    segment.segment.end_time_offset.nanos / 1e9)
        positions = '{}s to {}s'.format(start_time, end_time)
        confidence = segment.confidence
        print('\tSegment {}: {}'.format(i, positions))
        print('\tConfidence: {}'.format(confidence))
    print('\n')

 
from os.path import isfile, join
 
def convert_frames_to_video(pathIn,pathOut,fps):
    frame_array = []
    files = [f for f in os.listdir(pathIn) if isfile(join(pathIn, f))]
 
    #for sorting the file names properly
    files.sort(key = lambda x: int(x[5:-4]))
 
    for i in range(len(files)):
        filename=pathIn + files[i]
        #reading each files
        img = cv2.imread(filename)
        height, width, layers = img.shape
        size = (width,height)
        print(filename)
        #inserting the frames into an image array
        frame_array.append(img)
 
    out = cv2.VideoWriter(pathOut,cv2.VideoWriter_fourcc(*'DIVX'), fps, size)
 
    for i in range(len(frame_array)):
        # writing to a image array
        out.write(frame_array[i])
    out.release()
 
def main():
    pathIn= './data/'
    pathOut = 'video.avi'
    fps = 25.0
    convert_frames_to_video(pathIn, pathOut, fps)
 
if __name__=="__main__":
    main()
  
 
 
 ##google api acct##
 
 $pip install --upgrade googgle api-python-client google-auth-oathlib
 from__future__ import print_function
 import pickle
 import os.path
 from googleapiclient.discovery import build
 from google_auth.transport.request import request
 
 #if modifying these scopes, delete the file token. pickle.
 Scopes =['https:www.googleapis.com/auth/documents.readonly']
 
 # the ID of a sample document
 Document_ID = '???????????'
 def main():
    """shows basic usage of the Dos Api.
       Prints the title of a sample document."""
       creds= None
       # The file token.pickle stores the user's access and refresh tokens, and is
       # created automatically when the authorization flow completes for the first
       # time
 if os.path.exsist( 'token.pickle'):
    with open ('token.pickle' , 'rb') as token
        creds = pickle.load(token)
 # If there are no (valid) credentials available, let user log in.
 if not creds or not creds.valid:
    if creds and creds.expired and creds.refresh_token:
       creds.refresh(request())
    else:
         flow = InstalledAppFlow. from_client_secrect_file(
            'credentials.json' , SCOPES)
         creds = flow.run_local_server (port=0)
    # Save the credentials for the next run
    with open ('token.pickle' , 'wb') as token:
        pickle.dump (creds, token)
 service = build ('docs' , 'v1' ,credentials=creds)
 
 # Retreive the documents contents from the docsservice.
 document = service.documents().get(documentId=DOCUMENT_ID). execute()
 
 print ('the title of the document is: {}' .format(document.get ('title')))
 
 if __name__ == '__main__':
       main()
  
 $python quickstart.py 
 
def get_contact (contact, user_guide):
get_contact_url = "htpp:/" + "ip"
def create_contact (name, last_name, phone_number, description) + 
create_contact_url = "http://" + "ip" 

alerting a device
if img = no match
###create 3 pictures to pick from?
class device (models.model)+ (phone number)
serial number = models.uuidfeild() 
elif img + create_contact = true
 print "match found" 
 print cal
 
 #read persons name from google contact list
 import gtts from gTTS
 s=()  # persons name imported from contact list
 file = "file.mp3"
 # initialize tts, create mp3 and play
tts = gTTS(s, 'en')
tts.save(file)
os.system("mpg123 " + file)


stream.writer()
sd.play(myarray,fs)
sd.stop()

outputstream

Sock.shutdown(0)
Sock.close()

finally <cleanup>


